{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d9e61417",
      "metadata": {},
      "source": [
        "# Day 4\n",
        "\n",
        "## 0. Concepts\n",
        "\n",
        "- **Transformers**: Neural network architecture using self-attention to process sequences of tokens. Captures long-range dependencies efficiently. Model capacity is defined by **parameters (weights)** â€” larger models with billions of parameters can learn more complex patterns but require more compute.\n",
        "\n",
        "- **Tokens**: Discrete units of text or code that the model processes. Text is split into subword units, which determines how sequences are represented internally. Token counts are important because they affect both the model's **context window** and the cost of API usage.\n",
        "\n",
        "- **Context Window**: Maximum number of tokens the model can attend to in a single pass. Sequences longer than this limit are truncated or require special handling (e.g., sliding windows or chunking).\n",
        "\n",
        "- **API Costs**: Most LLM APIs charge per token processed or generated. Understanding tokenization helps estimate costs accurately and optimize requests.\n",
        "\n",
        "- **Tokenizing with Code**: Practice converting text or code into tokens programmatically. Measure token lengths and analyze how different inputs affect model performance and API usage.\n",
        "\n",
        "\n",
        "## 1. Tokenizing with code\n",
        "[tiktoken](https://github.com/openai/tiktoken) is a fast _BPE_ (byte-pair encoding) tokeniser for use with OpenAI's models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dc1c1d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "# tiktoken is OpenAI's tokenizer - it converts text into tokens\n",
        "# Different models use different tokenizers, so we specify which model\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
        "\n",
        "tokens = encoding.encode(\"Hi my name is Manu and I like migas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7632966c",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cce0c188",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's see what each token ID represents\n",
        "for token_id in tokens:\n",
        "    token_text = encoding.decode([token_id])\n",
        "    print(f\"{token_id} = {token_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98e3bbd2",
      "metadata": {},
      "outputs": [],
      "source": [
        "encoding.decode([164313])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "538efe61",
      "metadata": {},
      "source": [
        "## 2. The Illusion of \"memory\"\n",
        "\n",
        "Now let's see how LLMs actually work - they don't remember anything between calls!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83a4b3eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setting up Gemini\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "\n",
        "load_dotenv(override=True)\n",
        "\n",
        "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "base_url = os.getenv(\"GEMINI_BASE_URL\", \"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
        "model = os.getenv(\"GEMINI_MODEL\", \"gemini-3-flash-preview\")\n",
        "\n",
        "if not api_key:\n",
        "    raise ValueError(\"No GEMINI_API_KEY found in .env file\")\n",
        "else:\n",
        "    print(\"API key found!\")\n",
        "    client = OpenAI(base_url=base_url, api_key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97298fea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# First message (list of dicts)\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hi! I'm Manu!\"}\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(model=model, messages=messages)\n",
        "response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03569a40",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Second message that tries to ask a follow-up question\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"What's my name?\"}\n",
        "    ]\n",
        "\n",
        "response = client.chat.completions.create(model=model, messages=messages)\n",
        "response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3475a36d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wait, it doesn't know my name! That's because each API call is STATELESS\n",
        "# Every call is completely independent - the LLM has no memory between calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bce2208",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now including the full conversation history\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hi! I'm Manu!\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Hi Manu! How can I help you today?\"},\n",
        "    {\"role\": \"user\", \"content\": \"What's my name?\"}\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(model=model, messages=messages)\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "404462f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Key takeaway: LLMs are stateless!\n",
        "# Each API call is independent - no memory between calls\n",
        "# To create \"memory\", we pass the entire conversation history each time"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
